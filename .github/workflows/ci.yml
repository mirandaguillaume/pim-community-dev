concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

name: CI

on:
  pull_request:
    paths-ignore:
      - '**/*.md'
      - 'docs/**'
      - 'CHANGELOG*'
      - '.github/ISSUE_TEMPLATE/**'
      - '.github/dependabot.yml'
  push:
    branches:
      - master
    paths-ignore:
      - '**/*.md'
      - 'docs/**'
      - 'CHANGELOG*'
      - '.github/ISSUE_TEMPLATE/**'
      - '.github/dependabot.yml'
  workflow_dispatch:

env:
  CACHE_VERSION: v2
  YARN_REGISTRY: https://registry.yarnpkg.com
  COMPOSER_MEMORY_LIMIT: -1
  DOCKER_BUILDKIT: 1
  YARN_ENABLE_IMMUTABLE_INSTALLS: true
  # Runner label: set repository variable RUNNER_LABEL to 'self-hosted' in
  # Settings > Secrets and variables > Actions > Variables to use your own runner.
  # All jobs use self-hosted (5 runners on Hetzner CCX33).

defaults:
  run:
    shell: bash

# =============================================================================
# JOB DEPENDENCY GRAPH (with path filtering):
#
#   detect-changes ──────────────────────────(outputs: backend, frontend, ci)──┐
#     │                                                                        │
#   image ─────────────────────────────────────────────────────────────────────┐
#     │                                                                        │
#     ├──► prepare ──┬──► front-build [if frontend] ──┬──► playwright [if front]►│
#     │              │                                 │                       │
#     │              ├──► front-lint-unit [if frontend]──────────────────────►│
#     │              │                                                         │
#     │              ├──► lint-and-static-back [if backend] ────────────────►│
#     │              │                                                         │
#     │              ├──► phpspec [if backend] ────────────────────────────►│
#     │              │                                                         │
#     │              ├──► acceptance-back [if backend] ────────────────────►│
#     │              │                                                         │
#     │              ├──► data-migrations [if backend] ───────────────────►│
#     │              │                                                         │
#     │              ├──► deptrac [if backend] (all configs, 1 job) ──────►│
#     │              │                                                         │
#     │              ├──► phpunit (6 shards) ─────────────────────────────►│
#     │              │                                                         │
#     │              └──► behat-legacy (10 shards) ──────────────────────►│
#     │                                                                        │
#     └──► db-seed [if backend] ───────────────────────────────────────────────┤
#                                                                              │
#   ci-success accepts 'success' or 'skipped' (path-filtered jobs)    ────────┘
# =============================================================================

jobs:
  # ===========================================================================
  # DETECT CHANGES: Categorize changed files for conditional job execution
  # ===========================================================================
  detect-changes:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    timeout-minutes: 5
    outputs:
      backend: ${{ steps.filter.outputs.backend }}
      frontend: ${{ steps.filter.outputs.frontend }}
      ci: ${{ steps.filter.outputs.ci }}
    steps:
      - uses: actions/checkout@v4
      - uses: dorny/paths-filter@v3
        id: filter
        with:
          filters: |
            backend:
              - 'src/**/*.php'
              - 'tests/**/*.php'
              - 'spec/**/*.php'
              - 'upgrades/**'
              - 'config/**'
              - 'composer.json'
              - 'composer.lock'
              - 'phpstan*.neon'
              - 'phpspec.yml*'
              - 'phpunit.xml*'
              - '.php_cs*'
              - '.php-cs-fixer*'
              - 'deptrac-*.yaml'
              - 'components/**/*.php'
              - 'src/**/Resources/config/**/*.yml'
              - 'src/**/Resources/config/**/*.yaml'
              - 'src/**/Resources/translations/**'
            frontend:
              - 'front-packages/**'
              - 'frontend/**'
              - 'yarn.lock'
              - 'package.json'
              - 'webpack*.js'
              - 'babel.config.*'
              - 'jest.config.*'
              - '.eslintrc*'
              - 'tsconfig*.json'
              - 'playwright.config.ts'
              - 'src/**/*.ts'
              - 'src/**/*.tsx'
              - 'src/**/*.js'
              - 'src/**/*.jsx'
              - 'src/**/*.less'
            ci:
              - '.github/**'
              - 'docker-compose*.yml'
              - 'docker/**'
              - 'Makefile'
              - 'make-file/**'

  # ===========================================================================
  # BUILD PHP IMAGE (pushed to GHCR, not artifacts)
  # ===========================================================================
  image:
    uses: ./.github/workflows/reusable-image.yml
    secrets: inherit

  # ===========================================================================
  # PREPARE: Install dependencies, save caches
  # ===========================================================================
  prepare:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    timeout-minutes: 25
    needs: image
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-docker-compose

      - name: Login to GHCR
        run: echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin

      - name: Pull PHP image from GHCR
        run: |
          HASH=$(sha256sum Dockerfile | cut -c1-12)
          IMAGE="ghcr.io/${{ github.repository }}/php:df-${HASH}-8.2"
          docker pull "$IMAGE"
          docker tag "$IMAGE" akeneo/pim-php-dev:8.2

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: yarn
          cache-dependency-path: yarn.lock

      - name: Restore composer cache
        id: composer-cache
        uses: actions/cache/restore@v4
        with:
          path: vendor
          key: composer-${{ runner.os }}-${{ hashFiles('composer.lock') }}

      - name: Restore yarn cache
        id: yarn-cache
        uses: actions/cache/restore@v4
        with:
          path: |
            node_modules
            front-packages/akeneo-design-system/lib
            front-packages/shared/lib
            ~/.cache/ms-playwright
          key: yarn-${{ runner.os }}-${{ hashFiles('yarn.lock') }}

      - name: Prepare filesystem permissions
        run: |
          mkdir -p ~/.cache/yarn ~/.composer
          sudo chmod -R a+rwX ~/.cache ~/.composer
          sudo chmod -R a+rwX .
          sudo chmod -R a+rwX node_modules front-packages || true

      - name: Copy docker-compose override
        run: cp .github/scripts/docker-compose.override.yml.dist docker-compose.override.yml

      - name: Install JS dependencies
        run: |
          HOST_UID=$(id -u)
          HOST_GID=$(id -g)
          docker-compose run --rm --user "${HOST_UID}:${HOST_GID}" node sh -c "rm -f node_modules/.yarn-integrity && yarn install --frozen-lockfile --no-progress"

      - name: Build front packages (shared across downstream jobs)
        run: |
          HOST_UID=$(id -u)
          HOST_GID=$(id -g)
          docker-compose run --rm --user "${HOST_UID}:${HOST_GID}" node yarn packages:build

      - name: Install PHP dependencies
        run: |
          HOST_UID=$(id -u)
          HOST_GID=$(id -g)
          make YARN_RUN="docker-compose run --rm --user ${HOST_UID}:${HOST_GID} -e YARN_REGISTRY -e PUPPETEER_SKIP_CHROMIUM_DOWNLOAD=1 -e PUPPETEER_EXECUTABLE_PATH=/usr/bin/google-chrome node yarn --no-bin-links" dependencies

      - name: Upload front packages build
        uses: actions/upload-artifact@v4
        with:
          name: front-packages-lib
          path: |
            front-packages/akeneo-design-system/lib
            front-packages/shared/lib
            src/Akeneo/Pim/Structure/front/lib
            src/Akeneo/Tool/Bundle/MeasureBundle/front/lib
            src/Akeneo/Platform/Bundle/CatalogVolumeMonitoringBundle/front/lib
            src/Akeneo/Category/front/lib
            src/Akeneo/Platform/Bundle/UIBundle/Resources/workspaces/system/lib
            src/Akeneo/Platform/Job/front/process-tracker/lib
            src/Akeneo/Platform/Bundle/ImportExportBundle/front/lib
            src/Oro/Bundle/ConfigBundle/front/lib
            components/catalogs/mock/lib
            components/identifier-generator/front/lib
          retention-days: 1

      - name: Start MySQL (for Symfony console commands)
        run: |
          docker-compose up -d mysql
          echo "Waiting for MySQL to accept connections..."
          for i in $(seq 1 30); do
            docker-compose exec -T mysql mysql -uroot -proot -e "SELECT 1" >/dev/null 2>&1 && break
            [ "$i" -eq 30 ] && { echo "MySQL not ready after 30s"; docker-compose logs mysql | tail -20; exit 1; }
            sleep 1
          done
          echo "MySQL is ready (project: ${COMPOSE_PROJECT_NAME:-default})"

      - name: Pre-generate front assets (routes, require-paths, bundles)
        run: |
          docker-compose run --rm php sh -c "mkdir -p public/js public/bundles"
          docker-compose run --rm php php bin/console pim:installer:assets --clean --env=prod
          docker-compose run --rm php php bin/console fos:js-routing:dump --format=json --target=public/js/fos_js_routes.json
          docker-compose run --rm php sh -c "mkdir -p public/js && php bin/console pim:installer:dump-require-paths"

      - name: Upload pre-generated front assets
        uses: actions/upload-artifact@v4
        with:
          name: front-assets-pre
          path: public/
          retention-days: 1

      - name: Save composer cache
        if: steps.composer-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: vendor
          key: composer-${{ runner.os }}-${{ hashFiles('composer.lock') }}

      - name: Save yarn cache
        if: steps.yarn-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: |
            node_modules
            front-packages/akeneo-design-system/lib
            front-packages/shared/lib
            ~/.cache/ms-playwright
          key: yarn-${{ runner.os }}-${{ hashFiles('yarn.lock') }}

  # ===========================================================================
  # DB SEED: Create database dump for integration tests
  # ===========================================================================
  db-seed:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, image, detect-changes]
    if: needs.detect-changes.outputs.backend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 25
    steps:
      - uses: actions/checkout@v4

      - name: Restore DB seed from cache
        id: db-cache
        uses: actions/cache/restore@v4
        with:
          path: db-seed.sql
          key: db-seed-${{ hashFiles('upgrades/**/*.php', 'src/**/DataFixtures/**/*.php', 'src/**/DataFixtures/**/*.yaml') }}

      - name: Skip build if cache hit
        if: steps.db-cache.outputs.cache-hit == 'true'
        run: echo "DB seed restored from cache, skipping rebuild"

      - uses: ./.github/actions/setup-pim-job
        if: steps.db-cache.outputs.cache-hit != 'true'
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          yarn-cache: 'false'

      - name: Start services and create database
        if: steps.db-cache.outputs.cache-hit != 'true'
        run: |
          APP_ENV=test C='httpd mysql elasticsearch object-storage pubsub-emulator gcs-emulator' make up
          docker/wait_docker_up.sh
          APP_ENV=test make database

      - name: Dump database
        if: steps.db-cache.outputs.cache-hit != 'true'
        run: docker-compose exec -T mysql mysqldump -uroot -proot akeneo_pim_test > db-seed.sql

      - name: Save DB seed to cache
        if: steps.db-cache.outputs.cache-hit != 'true'
        uses: actions/cache/save@v4
        with:
          path: db-seed.sql
          key: db-seed-${{ hashFiles('upgrades/**/*.php', 'src/**/DataFixtures/**/*.php', 'src/**/DataFixtures/**/*.yaml') }}

      - name: Upload DB seed artifact
        uses: actions/upload-artifact@v4
        with:
          name: db-seed
          path: db-seed.sql
          retention-days: 1

  # ===========================================================================
  # FRONT-BUILD: Webpack build (produces artifacts for E2E tests)
  # ===========================================================================
  front-build:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, detect-changes]
    if: needs.detect-changes.outputs.frontend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          yarn-cache: 'true'

      - name: Install JS dependencies
        run: |
          HOST_UID=$(id -u)
          HOST_GID=$(id -g)
          docker-compose run --rm --user "${HOST_UID}:${HOST_GID}" node sh -c "rm -f node_modules/.yarn-integrity && yarn install --frozen-lockfile --no-progress"

      - name: Download pre-generated front assets
        uses: actions/download-artifact@v4
        with:
          name: front-assets-pre
          path: front-assets-pre

      - name: Restore pre-generated front assets
        run: |
          cp -a front-assets-pre/. public/
          sudo chmod -R a+rwX public/

      - name: Download front packages build
        uses: actions/download-artifact@v4
        with:
          name: front-packages-lib
          path: .

      - name: Build CSS (LESS compilation)
        run: make css

      - name: Build frontend (webpack)
        run: make javascript-dev-strict

      - name: Upload front build artifacts
        if: success()
        uses: actions/upload-artifact@v4
        with:
          name: front-build
          path: |
            public/css
            public/dist
            public/js/extensions.json
            public/js/fos_js_routes.json
            public/js/require-paths.js
            var/build/module-registry.js
          retention-days: 1

  # ===========================================================================
  # FRONT-LINT-UNIT: ESLint + Prettier + Jest unit tests (shared setup)
  # ===========================================================================
  front-lint-unit:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, detect-changes]
    if: needs.detect-changes.outputs.frontend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          yarn-cache: 'true'

      - name: Install JS dependencies
        run: |
          HOST_UID=$(id -u)
          HOST_GID=$(id -g)
          docker-compose run --rm --user "${HOST_UID}:${HOST_GID}" node sh -c "rm -f node_modules/.yarn-integrity && yarn install --frozen-lockfile --no-progress"

      - name: Download pre-generated front assets
        uses: actions/download-artifact@v4
        with:
          name: front-assets-pre
          path: front-assets-pre

      - name: Restore pre-generated front assets
        run: |
          cp -a front-assets-pre/. public/
          sudo chmod -R a+rwX public/

      - name: Download front packages build
        uses: actions/download-artifact@v4
        with:
          name: front-packages-lib
          path: .

      - name: Run ESLint and Prettier
        run: PIM_CONTEXT=test make lint-front

      - name: Generate models (JSON schemas)
        run: |
          HOST_UID=$(id -u)
          HOST_GID=$(id -g)
          docker-compose run --rm --user "${HOST_UID}:${HOST_GID}" node yarn generate-models

      - name: Run Jest unit tests
        run: PIM_CONTEXT=test make unit-front

  # ===========================================================================
  # LINT-AND-STATIC-BACK: PHPStan + php-cs-fixer + container lint + coupling
  # ===========================================================================
  lint-and-static-back:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, detect-changes]
    if: needs.detect-changes.outputs.backend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Restore PHPStan cache
        uses: actions/cache@v4
        with:
          path: .phpstan-cache
          key: phpstan-${{ runner.os }}-${{ hashFiles('phpstan.neon*', 'composer.lock') }}
          restore-keys: phpstan-${{ runner.os }}-

      - name: Start MySQL (needed by Symfony cache:warmup and lint:container)
        run: |
          docker-compose up -d mysql
          echo "Waiting for MySQL to accept connections..."
          for i in $(seq 1 30); do
            docker-compose exec -T mysql mysql -uroot -proot -e "SELECT 1" >/dev/null 2>&1 && break
            [ "$i" -eq 30 ] && { echo "MySQL not ready after 30s"; docker-compose logs mysql | tail -20; exit 1; }
            sleep 1
          done
          echo "MySQL is ready (project: ${COMPOSE_PROJECT_NAME:-default})"

      - name: Fix identifier generator CS (auto-fix before dry-run)
        continue-on-error: true
        run: docker-compose run --rm php php vendor/bin/php-cs-fixer fix --config=components/identifier-generator/back/tests/.php_cs.php --allow-risky=yes

      - name: Run PHPStan and php-cs-fixer
        run: |
          PIM_CONTEXT=test make deprecation-back
          PIM_CONTEXT=test make lint-back

      - name: Run static checks (container lint, coupling, translations)
        run: |
          PIM_CONTEXT=test make find-legacy-translations
          PIM_CONTEXT=test make static-back
          PIM_CONTEXT=test make coupling-back

  # ===========================================================================
  # PHPSPEC: Unit tests (fast, no external dependencies)
  # ===========================================================================
  phpspec:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, detect-changes]
    if: needs.detect-changes.outputs.backend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          pull-base-images: 'false'

      - name: Restore PHPSpec cache
        uses: actions/cache@v4
        with:
          path: .phpspec-cache
          key: phpspec-${{ runner.os }}-${{ hashFiles('phpspec.yml*', 'composer.lock') }}
          restore-keys: phpspec-${{ runner.os }}-

      - name: Run PHPSpec unit tests
        run: PIM_CONTEXT=test make unit-back

      - name: Upload PHPSpec results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: phpspec-results
          path: var/tests/phpspec
          retention-days: 3

      - name: Publish PHPSpec results
        if: always()
        uses: dorny/test-reporter@v1
        continue-on-error: true
        with:
          name: PHPSpec Results
          path: var/tests/phpspec/*.xml
          reporter: java-junit
          fail-on-error: false

  # ===========================================================================
  # ACCEPTANCE-BACK: Behat acceptance tests
  # ===========================================================================
  acceptance-back:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, detect-changes]
    if: needs.detect-changes.outputs.backend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 20
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Run Behat acceptance tests
        run: PIM_CONTEXT=test make acceptance-back

      - name: Upload acceptance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: acceptance-back-results
          path: |
            var/tests
            var/logs
          retention-days: 3

  # ===========================================================================
  # DEPTRAC: Architecture coupling analysis (all configs in 1 job)
  # ===========================================================================
  deptrac:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, image, detect-changes]
    if: needs.detect-changes.outputs.backend == 'true' || needs.detect-changes.outputs.ci == 'true'
    timeout-minutes: 15
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          create-var-dirs: 'false'
          pull-base-images: 'false'

      - name: Restore Deptrac cache
        uses: actions/cache@v4
        with:
          path: .deptrac-cache
          key: deptrac-all-${{ runner.os }}-${{ hashFiles('deptrac-*.yaml', 'composer.lock') }}
          restore-keys: deptrac-all-${{ runner.os }}-

      - name: Download Deptrac
        run: |
          curl -sSL https://github.com/qossmic/deptrac/releases/latest/download/deptrac.phar -o deptrac.phar
          chmod +x deptrac.phar

      - name: Run Deptrac (all configs)
        run: |
          for config in structure core job enrichment channel importexport; do
            echo "::group::Deptrac - ${config}"
            docker-compose run --rm php php /srv/pim/deptrac.phar analyze \
              --config-file=deptrac-${config}.yaml \
              --formatter=github-actions \
              --cache-file=/srv/pim/.deptrac-cache/${config}.cache \
              --no-progress
            echo "::endgroup::"
          done

  # ===========================================================================
  # DATA MIGRATIONS: Test database migrations
  # ===========================================================================
  data-migrations:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare]
    timeout-minutes: 30
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Run data migrations tests
        run: |
          APP_ENV=test C='httpd mysql elasticsearch object-storage pubsub-emulator' make up
          docker/wait_docker_up.sh
          APP_ENV=test make database
          PIM_CONTEXT=test make migration-back

      - name: Upload migration results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: migration-results
          path: |
            var/tests/phpunit
            var/logs
          retention-days: 3

  # ===========================================================================
  # PHPUNIT: Integration and E2E tests (sharded)
  # ===========================================================================
  phpunit:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, db-seed]
    timeout-minutes: 90
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4, 5, 6]
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download DB seed
        uses: actions/download-artifact@v4
        with:
          name: db-seed
          path: seed

      - name: Cleanup stale containers
        run: docker-compose down --remove-orphans 2>/dev/null || true

      - name: Setup test database
        run: |
          APP_ENV=test C='httpd mysql elasticsearch object-storage pubsub-emulator gcs-emulator' make up
          docker/wait_docker_up.sh
          .github/scripts/setup-test-db.sh akeneo_pim_test seed/db-seed.sql
          docker-compose run --rm php php bin/console akeneo:elasticsearch:reset-indexes --env=test

      - name: Install Symfony assets
        run: docker-compose run --rm php php bin/console pim:installer:assets --symlink --clean --env=test

      - name: Restore PHPUnit timing data
        uses: actions/cache/restore@v4
        with:
          path: .test-timings/phpunit.json
          key: test-timings-phpunit-${{ github.ref }}-
          restore-keys: |
            test-timings-phpunit-refs/heads/master-

      - name: Run PHPUnit tests (shard ${{ matrix.shard }}/6)
        run: |
          .github/scripts/run_phpunit.sh . .github/scripts/find_phpunit.php \
            PIM_Integration_Test,Akeneo_Connectivity_Connection_Integration,Akeneo_Communication_Channel_Integration,Job_Integration_Test,Channel_Integration_Test,Identifier_Generator_PhpUnit,Installer_Integration_Test,End_to_End
        env:
          PHPUNIT_SHARD: ${{ matrix.shard }}
          PHPUNIT_TOTAL_SHARDS: 6
          PHPUNIT_TIMING_FILE: .test-timings/phpunit.json

      - name: Collect PHPUnit timings
        if: always()
        run: |
          .github/scripts/collect-timings.sh \
            "var/tests/phpunit/**/*.xml" \
            ".test-timings/phpunit-shard-${{ matrix.shard }}.json" \
            ".test-timings/phpunit.json"

      - name: Upload PHPUnit timing data
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: phpunit-timings-${{ matrix.shard }}
          path: .test-timings/phpunit-shard-${{ matrix.shard }}.json
          retention-days: 1

      - name: Upload PHPUnit results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: phpunit-results-${{ matrix.shard }}
          path: |
            var/tests/phpunit
            var/logs
          retention-days: 3

      - name: Publish PHPUnit results
        if: always()
        uses: dorny/test-reporter@v1
        continue-on-error: true
        with:
          name: PHPUnit Results (Shard ${{ matrix.shard }})
          path: var/tests/phpunit/**/*.xml
          reporter: java-junit
          fail-on-error: false

  # ===========================================================================
  # BEHAT LEGACY: End-to-end legacy tests (sharded)
  # ===========================================================================
  behat-legacy:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, db-seed, front-build]
    timeout-minutes: 120
    strategy:
      fail-fast: false
      matrix:
        shard: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}

      - name: Download DB seed
        uses: actions/download-artifact@v4
        with:
          name: db-seed
          path: seed

      - name: Download front build artifact
        uses: actions/download-artifact@v4
        with:
          name: front-build
          path: front-build

      - name: Compute Behat suite
        run: |
          BRANCH_NAME="${GITHUB_HEAD_REF:-${GITHUB_REF##*/}}"
          TESTSUITE=$(echo "${BRANCH_NAME}" | sed -e 's/^.*-\([^-]*\)$/\1/g')
          if ! [[ "${TESTSUITE}" =~ ^(weasel|chipmunk|raccoon)$ ]]; then
            TESTSUITE="all"
          fi
          echo "TESTSUITE=${TESTSUITE}" >> $GITHUB_ENV

      - name: Cleanup stale containers
        run: docker-compose down --remove-orphans 2>/dev/null || true

      - name: Setup test database
        run: |
          DB_NAME=$(grep '^APP_DATABASE_NAME=' .env.behat 2>/dev/null | cut -d= -f2 || echo "akeneo_pim_test")
          DB_NAME=${DB_NAME:-akeneo_pim_test}
          APP_ENV=behat C='httpd mysql elasticsearch object-storage selenium pubsub-emulator' make up
          docker/wait_docker_up.sh
          .github/scripts/setup-test-db.sh "${DB_NAME}" seed/db-seed.sql
          docker-compose run --rm php php bin/console akeneo:elasticsearch:reset-indexes --env=test

      - name: Install front assets
        run: |
          mkdir -p public/css public/dist public/js var/build
          cp -a front-build/public/css/. public/css/ 2>/dev/null || true
          cp -a front-build/public/dist/. public/dist/ 2>/dev/null || true
          cp -a front-build/public/js/. public/js/ 2>/dev/null || true
          cp -a front-build/var/build/module-registry.js var/build/ 2>/dev/null || true
          sudo chmod -R 777 public/
          docker-compose run --rm php php bin/console pim:installer:assets --symlink --env=behat
          sudo chown -R $(id -u):$(id -g) public/

      - name: Warmup Symfony cache
        run: docker-compose run --rm php php bin/console cache:warmup --env=behat

      - name: Restore Behat timing data
        uses: actions/cache/restore@v4
        with:
          path: .test-timings/behat.json
          key: test-timings-behat-${{ github.ref }}-
          restore-keys: |
            test-timings-behat-refs/heads/master-

      - name: Run Behat legacy tests (shard ${{ matrix.shard }}/10)
        run: |
          export BEHAT_SPLIT="${{ matrix.shard }}/10"
          export BEHAT_TIMING_FILE=".test-timings/behat.json"
          PIM_CONTEXT=test make end-to-end-legacy SUITE=${TESTSUITE}

      - name: Archive behat artifacts
        if: always()
        run: tar -czf behat-artifacts-${{ matrix.shard }}.tar.gz var/tests/behat var/tests/screenshots var/logs 2>/dev/null || true

      - name: Upload behat artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: behat-results-${{ matrix.shard }}
          path: behat-artifacts-${{ matrix.shard }}.tar.gz
          retention-days: 3

  # ===========================================================================
  # PLAYWRIGHT: Frontend E2E tests
  # ===========================================================================
  playwright:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [prepare, front-build, detect-changes]
    if: >-
      (needs.detect-changes.outputs.frontend == 'true' || needs.detect-changes.outputs.ci == 'true') &&
      needs.prepare.result == 'success' &&
      needs.front-build.result == 'success'
    timeout-minutes: 45
    steps:
      - uses: actions/checkout@v4

      - uses: ./.github/actions/setup-pim-job
        with:
          github-token: ${{ secrets.GITHUB_TOKEN }}
          yarn-cache: 'true'

      - name: Download front build artifact
        uses: actions/download-artifact@v4
        with:
          name: front-build
          path: front-build

      - name: Install node dependencies (if cache miss)
        run: |
          if [ ! -f node_modules/.bin/playwright ]; then
            echo "Cache miss — running yarn install"
            yarn install --frozen-lockfile
          fi

      - name: Install Playwright browsers
        run: ./node_modules/.bin/playwright install --with-deps chromium

      - name: Setup and run Playwright
        run: |
          DB_NAME=$(grep '^APP_DATABASE_NAME=' .env | cut -d= -f2 || echo "akeneo_pim")
          DB_NAME=${DB_NAME:-akeneo_pim}
          APP_ENV=prod C='httpd mysql elasticsearch object-storage pubsub-emulator' make up
          docker/wait_docker_up.sh
          APP_ENV=prod O="--catalog src/Akeneo/Platform/Installer/back/src/Infrastructure/Symfony/Resources/fixtures/icecat_demo_dev" make database
          .github/scripts/setup-test-db.sh "${DB_NAME}" "" .github/scripts/behat-extra-tables.sql
          # Reset Elasticsearch indexes and re-index products so the product grid works
          docker-compose run --rm php php bin/console akeneo:elasticsearch:reset-indexes --env=prod
          docker-compose run --rm php php bin/console pim:product:index --all --env=prod
          docker-compose run --rm php php bin/console pim:product-model:index --all --env=prod
          # Restore front build artifacts FIRST (CSS, dist, JS, module-registry)
          mkdir -p public/css public/dist public/js var/build
          cp -a front-build/public/css/. public/css/ 2>/dev/null || true
          cp -a front-build/public/dist/. public/dist/ 2>/dev/null || true
          cp -a front-build/public/js/. public/js/ 2>/dev/null || true
          cp -a front-build/var/build/module-registry.js var/build/ 2>/dev/null || true
          # Fix ownership so Docker container can write to public/ during assets install
          sudo chmod -R 777 public/
          # Install Symfony assets WITHOUT --clean (adds bundles + translations without wiping)
          docker-compose run --rm php php bin/console pim:installer:assets --symlink --env=prod
          # Restore sane ownership for the host runner
          sudo chown -R $(id -u):$(id -g) public/
          # Warm Symfony cache so first request doesn't timeout
          docker-compose run --rm php php bin/console cache:warmup --env=prod
          # Verify front build assets are present
          echo "=== Front asset verification ==="
          for f in public/dist/vendor.min.js public/dist/main.min.js public/js/extensions.json public/css/pim.css; do
            if [ -f "$f" ]; then echo "OK: $f ($(wc -c < "$f") bytes)"; else echo "MISSING: $f"; fi
          done
          # Wait for httpd to serve a fully-rendered login page (proves kernel + Twig + assets work)
          echo "Waiting for login page to be ready..."
          for i in $(seq 1 24); do
            if curl -sS http://localhost:${DOCKER_PORT_HTTP:-8080}/user/login 2>/dev/null | grep -q '_username'; then
              echo "Login page ready after $((i * 5))s"
              break
            fi
            if [ "$i" -eq 24 ]; then
              echo "ERROR: Login page not ready after 120s. Response:"
              curl -sS http://localhost:${DOCKER_PORT_HTTP:-8080}/user/login 2>&1 | head -50
              exit 1
            fi
            sleep 5
          done
          # Verify JS assets are served correctly via HTTP
          echo "=== HTTP asset check ==="
          curl -sI http://localhost:${DOCKER_PORT_HTTP:-8080}/dist/vendor.min.js 2>&1 | head -3
          curl -sI http://localhost:${DOCKER_PORT_HTTP:-8080}/js/extensions.json 2>&1 | head -3
          PIM_URL=http://localhost:${DOCKER_PORT_HTTP:-8080} make end-to-end-front

      - name: Upload Playwright report
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: playwright-report
          path: |
            playwright-report/
            test-results/
          retention-days: 3

  # ===========================================================================
  # SAVE TIMINGS: Merge shard timing data into cache (master only)
  # ===========================================================================
  save-timings:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs: [phpunit, behat-legacy]
    if: always() && github.ref == 'refs/heads/master'
    timeout-minutes: 5
    steps:
      - uses: actions/checkout@v4
        with:
          sparse-checkout: .github/scripts

      - name: Download all PHPUnit timing artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: phpunit-timings-*
          path: timing-shards/phpunit
          merge-multiple: true

      - name: Merge and save PHPUnit timings
        run: |
          mkdir -p .test-timings
          python3 -c "
          import json, glob, os
          merged = {}
          for f in glob.glob('timing-shards/phpunit/*.json'):
              with open(f) as fh:
                  data = json.load(fh)
                  merged.update(data)
          os.makedirs('.test-timings', exist_ok=True)
          with open('.test-timings/phpunit.json', 'w') as fh:
              json.dump(merged, fh, indent=2, sort_keys=True)
          print(f'Merged {len(merged)} PHPUnit timing entries')
          "

      - name: Save PHPUnit timing cache
        uses: actions/cache/save@v4
        with:
          path: .test-timings/phpunit.json
          key: test-timings-phpunit-${{ github.ref }}-${{ github.run_number }}

  # ===========================================================================
  # CI SUCCESS: Final gate job
  # ===========================================================================
  ci-success:
    runs-on: ${{ vars.RUNNER_LABEL || 'ubuntu-latest' }}
    needs:
      - detect-changes
      - front-build
      - front-lint-unit
      - lint-and-static-back
      - phpspec
      - acceptance-back
      - deptrac
      - phpunit
      - data-migrations
      - behat-legacy
      - playwright
    if: always()
    timeout-minutes: 5
    steps:
      - name: Check all jobs passed
        run: |
          echo "=== Path filter results ==="
          echo "  backend changes: ${{ needs.detect-changes.outputs.backend }}"
          echo "  frontend changes: ${{ needs.detect-changes.outputs.frontend }}"
          echo "  ci changes: ${{ needs.detect-changes.outputs.ci }}"
          echo ""
          echo "=== Job results ==="
          echo "  front-build: ${{ needs.front-build.result }}"
          echo "  front-lint-unit: ${{ needs.front-lint-unit.result }}"
          echo "  lint-and-static-back: ${{ needs.lint-and-static-back.result }}"
          echo "  phpspec: ${{ needs.phpspec.result }}"
          echo "  acceptance-back: ${{ needs.acceptance-back.result }}"
          echo "  deptrac: ${{ needs.deptrac.result }}"
          echo "  phpunit: ${{ needs.phpunit.result }}"
          echo "  data-migrations: ${{ needs.data-migrations.result }}"
          echo "  behat-legacy: ${{ needs.behat-legacy.result }}"
          echo "  playwright: ${{ needs.playwright.result }}"

          # Each job must be either 'success' or 'skipped' (path-filtered)
          results=(
            "${{ needs.front-build.result }}"
            "${{ needs.front-lint-unit.result }}"
            "${{ needs.lint-and-static-back.result }}"
            "${{ needs.phpspec.result }}"
            "${{ needs.acceptance-back.result }}"
            "${{ needs.deptrac.result }}"
            "${{ needs.phpunit.result }}"
            "${{ needs.data-migrations.result }}"
            "${{ needs.behat-legacy.result }}"
            "${{ needs.playwright.result }}"
          )

          failed=false
          for result in "${results[@]}"; do
            if [[ "$result" != "success" && "$result" != "skipped" ]]; then
              failed=true
            fi
          done

          if [[ "$failed" == "true" ]]; then
            echo "::error::One or more CI jobs failed"
            exit 1
          fi

          echo "All CI jobs passed successfully!"
